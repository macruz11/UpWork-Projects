{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8323485b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-02T22:33:18.217174Z",
     "iopub.status.busy": "2024-10-02T22:33:18.216645Z",
     "iopub.status.idle": "2024-10-02T22:33:19.195859Z",
     "shell.execute_reply": "2024-10-02T22:33:19.194600Z"
    },
    "papermill": {
     "duration": 0.988005,
     "end_time": "2024-10-02T22:33:19.198891",
     "exception": false,
     "start_time": "2024-10-02T22:33:18.210886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pdfsss/pdf2.pdf\n",
      "/kaggle/input/pdfsss/pdf1.pdf\n",
      "/kaggle/input/pdfsss/pdf3.pdf\n",
      "/kaggle/input/pdfsss2/pdf2.pdf\n",
      "/kaggle/input/pdfsss2/pdf4.pdf\n",
      "/kaggle/input/pdfsss2/pdf1.pdf\n",
      "/kaggle/input/pdfsss2/pdf3.pdf\n",
      "/kaggle/input/pdfsss3/pdf2.pdf\n",
      "/kaggle/input/pdfsss3/pdf5.pdf\n",
      "/kaggle/input/pdfsss3/pdf4.pdf\n",
      "/kaggle/input/pdfsss3/pdf8.pdf\n",
      "/kaggle/input/pdfsss3/pdf1.pdf\n",
      "/kaggle/input/pdfsss3/pdf11.pdf\n",
      "/kaggle/input/pdfsss3/pdf7.pdf\n",
      "/kaggle/input/pdfsss3/pdf10.pdf\n",
      "/kaggle/input/pdfsss3/pdf9.pdf\n",
      "/kaggle/input/pdfsss3/pdf3.pdf\n",
      "/kaggle/input/pdfsss3/pdf6.pdf\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbe8128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T22:33:19.207460Z",
     "iopub.status.busy": "2024-10-02T22:33:19.206753Z",
     "iopub.status.idle": "2024-10-02T22:33:36.476695Z",
     "shell.execute_reply": "2024-10-02T22:33:36.475162Z"
    },
    "papermill": {
     "duration": 17.277678,
     "end_time": "2024-10-02T22:33:36.479884",
     "exception": false,
     "start_time": "2024-10-02T22:33:19.202206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.5)\r\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\r\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (10.3.0)\r\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\r\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\r\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\r\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d7bdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T22:33:36.494434Z",
     "iopub.status.busy": "2024-10-02T22:33:36.493920Z",
     "iopub.status.idle": "2024-10-02T22:33:49.339605Z",
     "shell.execute_reply": "2024-10-02T22:33:49.338467Z"
    },
    "papermill": {
     "duration": 12.856432,
     "end_time": "2024-10-02T22:33:49.342841",
     "exception": false,
     "start_time": "2024-10-02T22:33:36.486409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import pdfplumber\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Step 1: Function to extract text from PDF and save to CSV using pdfplumber\n",
    "def pdf_to_csv(pdf_file, csv_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Split text into rows using new lines\n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        # Save the extracted text to a CSV file\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for line in lines:\n",
    "                writer.writerow([line])  # Each line becomes a row in the CSV file\n",
    "\n",
    "# Step 2: Transform the DataFrame based on specific conditions\n",
    "def transform_dataframe(df):\n",
    "    pd_rows, pd_cols = df.shape\n",
    "\n",
    "    # Ensure there are enough rows and columns for the transformations\n",
    "    def validate_df(row_num, col_num):\n",
    "        if pd_rows > row_num and pd_cols > col_num:\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: DataFrame does not have enough rows ({pd_rows}) or columns ({pd_cols}) for the condition at row {row_num} and column {col_num}.\")\n",
    "            return False\n",
    "\n",
    "    # Step 1: Check for \"CANOPY\" in row 1, column 0\n",
    "    if validate_df(1, 0) and df.iloc[0, 0] == \"CANOPY\":\n",
    "        print(f\"CANOPY DataFrame dimensions: {pd_rows} rows, {pd_cols} columns\")\n",
    "        \n",
    "        # Insert a blank row and a new row with specified column names\n",
    "        blank_row = pd.Series([''] * pd_cols, index=df.columns)\n",
    "        df = pd.concat([df, blank_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        column_names = [\n",
    "            'INSURANCE_PROVIDER', 'CHEQUE_#', 'CHEQUE_DATE', 'BATCH_ID', 'DATE_FILLED', \n",
    "            'CLAIM_#', 'MEMBER_ID', 'NAME', 'BENEFIT', 'REF._NO', 'STAT', 'CHARGED', \n",
    "            'COPAY', 'EXCLUDED', 'DEDUCTIBLE', '%', 'PAYABLE', 'FEE', 'GCT', 'TOTAL', \n",
    "            'REJECT_REASON', 'REMARK_CODE'\n",
    "        ]\n",
    "        column_names_row = pd.Series(column_names + [''] * (pd_cols - len(column_names)), index=df.columns)\n",
    "        df = pd.concat([df, column_names_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        # Copy the data from rows 5 to pd_rows-5 and columns 0 to 12\n",
    "        if pd_rows > 10:  # Ensure there are enough rows for this slice\n",
    "            data_to_copy = df.iloc[5:pd_rows-5, 0:13]\n",
    "            df = pd.concat([df, data_to_copy], ignore_index=True)\n",
    "        \n",
    "        # Display the first few rows after transformation\n",
    "        print(\"First few rows after CANOPY transformation:\")\n",
    "        print(df.head())\n",
    "\n",
    "    # Step 2: Check for \"GUARDIAN\" in row 7, column 0\n",
    "    if validate_df(7, 0) and df.iloc[7, 0] == \"GUARDIAN\":\n",
    "        print(f\"GUARDIAN DataFrame dimensions: {pd_rows} rows, {pd_cols} columns\")\n",
    "        \n",
    "        # Insert a blank row and a new row with specified column names\n",
    "        blank_row = pd.Series([''] * pd_cols, index=df.columns)\n",
    "        df = pd.concat([df, blank_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        column_names = [\n",
    "            'INSURANCE_PROVIDER', 'CHEQUE_#', 'CHEQUE_DATE', 'BATCH_ID', 'DATE_FILLED', \n",
    "            'CLAIM_#', 'MEMBER_ID', 'NAME', 'BENEFIT', 'REF._NO', 'STAT', 'CHARGED', \n",
    "            'COPAY', 'EXCLUDED', 'DEDUCTIBLE', '%', 'PAYABLE', 'FEE', 'GCT', 'TOTAL', \n",
    "            'REJECT_REASON', 'REMARK_CODE'\n",
    "        ]\n",
    "        column_names_row = pd.Series(column_names + [''] * (pd_cols - len(column_names)), index=df.columns)\n",
    "        df = pd.concat([df, column_names_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        # Copy the data from rows 10 to pd_rows-7 and columns 0 to 13\n",
    "        if pd_rows > 17:  # Ensure there are enough rows for this slice\n",
    "            data_to_copy = df.iloc[10:pd_rows-7, 0:14]\n",
    "            df = pd.concat([df, data_to_copy], ignore_index=True)\n",
    "        \n",
    "        # Display the first few rows after transformation\n",
    "        print(\"First few rows after GUARDIAN transformation:\")\n",
    "        print(df.head())\n",
    "\n",
    "    # Step 3: Check for \"Sagicor\" in row 1, column 0\n",
    "    if validate_df(1, 0) and df.iloc[0, 0] == \"Sagicor\":\n",
    "        print(f\"Sagicor DataFrame dimensions: {pd_rows} rows, {pd_cols} columns\")\n",
    "        \n",
    "        # Insert a blank row and a new row with specified column names\n",
    "        blank_row = pd.Series([''] * pd_cols, index=df.columns)\n",
    "        df = pd.concat([df, blank_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        column_names = [\n",
    "            'INSURANCE_PROVIDER', 'CHEQUE_#', 'CHEQUE_DATE', 'BATCH_ID', 'DATE_FILLED', \n",
    "            'CLAIM_#', 'MEMBER_ID', 'NAME', 'BENEFIT', 'REF._NO', 'STAT', 'CHARGED', \n",
    "            'COPAY', 'EXCLUDED', 'DEDUCTIBLE', '%', 'PAYABLE', 'FEE', 'GCT', 'TOTAL', \n",
    "            'REJECT_REASON', 'REMARK_CODE'\n",
    "        ]\n",
    "        column_names_row = pd.Series(column_names + [''] * (pd_cols - len(column_names)), index=df.columns)\n",
    "        df = pd.concat([df, column_names_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "        # Delete all rows that start with specified words\n",
    "        rows_to_delete = [\"PAGE\", \"PROPHYSIO\", \"134CONSTANTSPRINGROAD\", \"KINGSTON10\", \n",
    "                          \"PROVIDERTAXID:\", \"28-48BarbadosAvenue.,Kingston5,JamaicaW.I.\", \n",
    "                          \"Telephone:(876)929-8920-9\", \"YOU\", \"-\"]\n",
    "        \n",
    "        df = df[~df.iloc[:, 0].str.startswith(tuple(rows_to_delete), na=False)]\n",
    "        \n",
    "        # Delete all blank rows\n",
    "        df = df.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "        # Display the first few rows after transformation\n",
    "        print(\"First few rows after Sagicor transformation:\")\n",
    "        print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 3: Process PDFs to Excel and then transform DataFrames\n",
    "def process_pdfs_to_excel(pdf_files, output_directory, excel_file):\n",
    "    # Create a workbook to save transformed CSV data\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        # Convert PDF to CSV\n",
    "        csv_file = os.path.join(output_directory, os.path.basename(pdf_file).replace('.pdf', '.csv'))\n",
    "        pdf_to_csv(pdf_file, csv_file)\n",
    "\n",
    "        # Read the generated CSV into a DataFrame\n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "        # Apply transformations based on conditions\n",
    "        transformed_df = transform_dataframe(df)\n",
    "\n",
    "        # Append DataFrame to Excel sheet\n",
    "        for r in transformed_df.itertuples(index=False):\n",
    "            ws.append(r)\n",
    "    \n",
    "    # Save the workbook\n",
    "    wb.save(excel_file)\n",
    "\n",
    "# Paths to the PDF files (replace with your actual file paths)\n",
    "pdf_files = [\n",
    "    \"/kaggle/input/pdfsss3/pdf1.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf2.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf3.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf4.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf5.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf6.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf7.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf8.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf9.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf10.pdf\",\n",
    "    \"/kaggle/input/pdfsss3/pdf11.pdf\"\n",
    "]\n",
    "\n",
    "# Directory where the converted CSVs will be saved\n",
    "output_directory = \"/kaggle/working/\"\n",
    "\n",
    "# Path to the final Excel file where the data will be stored\n",
    "excel_file = \"/kaggle/working/imported_data.xlsx\"\n",
    "\n",
    "# Process the PDF files and export to Excel\n",
    "process_pdfs_to_excel(pdf_files, output_directory, excel_file)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5790577,
     "sourceId": 9512642,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5795271,
     "sourceId": 9518762,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5801120,
     "sourceId": 9526538,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.172888,
   "end_time": "2024-10-02T22:33:50.171191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-02T22:33:14.998303",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
